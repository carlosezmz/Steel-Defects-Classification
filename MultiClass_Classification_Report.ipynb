{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steel Defects Classification\n",
    "\n",
    "In this section we are build a multi-class to classifier to classify the type steel defect shown in an image. To mitigate the class-imbalance, during training we will augment the images and for every batch augmented we will over sample the minority classes to have better representation. To evaluate the performance of the model, we expecct the precision, recall and f1 scores to be similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# main libraries\n",
    "\n",
    "# dataset\n",
    "from steel_defects import steel_defects\n",
    "\n",
    "# model\n",
    "from defects_classifier import defects_classifier\n",
    "\n",
    "# batch generator over sampler\n",
    "from BalancedDataGenerator import BalancedDataGenerator\n",
    "\n",
    "# image generator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# model training helpers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Steel Defects Dataset\n",
    "\n",
    "The images are preprocessed into a shape of 150 by 150 to gray scale, as a way to reduce the number of features. Noticed when each dataset is loaded into memory it also shows the number of each class. Clasnumber 3 has approximately 75 percent of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "steel = steel_defects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dir = '/Users/carlostavarez/Desktop/imgs_multiClass/train'\n",
    "tst_dir = '/Users/carlostavarez/Desktop/imgs_multiClass/test'\n",
    "val_dir = '/Users/carlostavarez/Desktop/imgs_multiClass/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630/630 [00:02<00:00, 261.01it/s]\n",
      "100%|██████████| 157/157 [00:00<00:00, 279.19it/s]\n",
      "100%|██████████| 3850/3850 [00:14<00:00, 266.88it/s]\n",
      "100%|██████████| 422/422 [00:01<00:00, 245.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "tr_imgs, tr_lbls = steel.load_defects(trn_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 256.77it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 265.21it/s]\n",
      "100%|██████████| 433/433 [00:01<00:00, 265.37it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 249.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# validation data\n",
    "vl_imgs, vl_lbls = steel.load_defects(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 233.16it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 257.61it/s]\n",
      "100%|██████████| 476/476 [00:01<00:00, 248.38it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 243.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "ts_imgs, ts_lbls = steel.load_defects(tst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "To reduce the memory foot-print, to train the model we will use an image generator from keras. As mentioned above, an additional piece of code was added to balance the image generated during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps per epoch 481\n"
     ]
    }
   ],
   "source": [
    "tr_generator = ImageDataGenerator(rescale=1.0/255, \n",
    "                            brightness_range=(0.2, 0.7), \n",
    "                            horizontal_flip=True, \n",
    "                            vertical_flip=True)\n",
    "\n",
    "ts_generator = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "ts_gen = ts_generator.flow(ts_imgs, ts_lbls, batch_size=32, seed=42)\n",
    "vl_gen = ts_generator.flow(vl_imgs, vl_lbls, batch_size=32, seed=42)\n",
    "\n",
    "bgen = BalancedDataGenerator(tr_imgs, tr_lbls, tr_generator, batch_size=32)\n",
    "\n",
    "print('Number of steps per epoch {}'.format(bgen.steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "In this part we will train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classifier = defects_classifier.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 148, 148, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 146, 146, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_1 (Spatial (None, 73, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 71, 71, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_2 (Spatial (None, 35, 35, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 33, 33, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               33554944  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 33,944,964\n",
      "Trainable params: 33,944,900\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_list = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=0, factor=0.04, min_lr=0.001),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 54/481 [==>...........................] - ETA: 9:15 - loss: 1.3071 - accuracy: 0.3328"
     ]
    }
   ],
   "source": [
    "history = model_classifier.fit_generator(bgen, \n",
    "                                           steps_per_epoch=481,\n",
    "                                           validation_data=vl_gen,\n",
    "                                           validation_steps=len(vl_imgs)//32,\n",
    "                                           epochs=50,\n",
    "                                           callbacks=call_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ahora si')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
